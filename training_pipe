
from models import *
from game import Game, GameOptions
from gym.wrappers import FlattenObservation
from trainer import Trainer
import numpy as np
import matplotlib.pyplot as plt
import time

# if gpu is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def plot_rewards_and_durations(episode_durations, rewards):
        #plot episode durations
    plt.figure(1)
    plt.clf()
    plt.title('Episode durations')
    plt.xlabel('Episode')
    plt.ylabel('Duration')
    plt.plot(episode_durations)
    
    plt.figure(2)
    plt.clf()
    plt.title('Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.plot(rewards)
    plt.show()


def run_training(options, model_class, id, model_weights=None, plot=True):
    env = FlattenObservation(Game(render_mode=None, options=options, render_fps=1000)) 
    obs, _ = env.reset()
    action_shape = (-1, len(env.action_space.nvec), np.max(env.action_space.nvec))

    # target_model is the Q network we are training
    target_model = model_class(obs.shape[0], np.sum(env.action_space.nvec), action_shape).to(device)
    # aux_model helps us do soft updates
    aux_model = model_class(obs.shape[0], np.sum(env.action_space.nvec), action_shape).to(device)

    if model_weights is not None:
        target_model.load_state_dict(torch.load(model_weights))
        aux_model.load_state_dict(torch.load(model_weights))

    optimizer = optim.RMSprop(target_model.parameters())

    trainer = Trainer(target_model=target_model, 
                        aux_model=aux_model, 
                        env=env, 
                        optimizer=optimizer, 
                        action_shape=action_shape, 
                        device=device) #, gamma=0.99, batch_size=128, memory_size=10000, target_update=10, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=500)
    
    durations, rewards, success = trainer.do_train(100, max_step_per_episode=10000, target_max_steps=100, target_success_window=10)
    
    weights_path = ""

    if success:
        if plot:
            plot_rewards_and_durations(durations, rewards)
        weights_path = "checkpoints/model_{}_{}.pt".format(id,time.time())
        torch.save(target_model.state_dict(), weights_path)
    
    return weights_path, success



options = GameOptions(  height=600, 
                        width=400, 
                        n_obstacles=1, 
                        n_turrets=1, 
                        max_projectiles_per_turret=2, 
                        fire_turret_step_delay=0
                        )

while True:
    weights_path_1, success = run_training(options, SimpleLinearModel, "600x600_1t_2p_1o_0d")
    if success: break

options = GameOptions(  height=600, 
                        width=400, 
                        n_obstacles=1, 
                        n_turrets=1, 
                        max_projectiles_per_turret=2, 
                        fire_turret_step_delay=100
                        )

while True:
    weights_path_2, success = run_training(options, SimpleLinearModel, "600x600_1t_2p_1o_100d" , weights_path_1)
    if success: break
